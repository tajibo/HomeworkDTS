{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from gensim import corpora, models\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "import re\n",
        "from numpy import argmax"
      ],
      "metadata": {
        "id": "GOU3O26IxIVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoaNr-SSt3C3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914a770f-124b-4a9e-8eaa-bd2d52b63f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тема 1:\n",
            "['россии', 'где', 'исторический', 'музей', 'которого', 'самых', 'из', 'европе', 'музеев', 'один']\n",
            "\n",
            "Тема 2:\n",
            "['из', 'музей', 'здесь', 'музеев', 'один', 'мира', 'им', 'одно', 'москве', 'коллекций']\n",
            "\n",
            "Тема 3:\n",
            "['искусства', 'это', 'центр', 'самых', 'для', 'как', 'из', 'россии', 'музей', 'современного']\n",
            "\n",
            "Тема 4:\n",
            "['она', 'не', 'музей', 'но', 'музеем', 'самым', 'является', 'москве', 'котором', 'достопримечательностей']\n",
            "\n",
            "Тема 5:\n",
            "['на', 'музей', 'расположенный', 'был', 'москвы', 'от', 'км', 'крупным', 'является', 'самым']\n",
            "\n",
            "Тема LSA 1:\n",
            "['музей', 'из', 'самых', 'как', 'не', 'на', 'россии', 'был', 'для', 'музеев']\n",
            "\n",
            "Тема LSA 2:\n",
            "['для', 'был', 'музей', 'как', 'на', 'россии', 'по', 'хранения', 'центре', 'искусств']\n",
            "\n",
            "Тема LSA 3:\n",
            "['на', 'где', 'расположенный', 'москве', 'одно', 'является', 'самым', 'петра', 'крупным', 'исторический']\n",
            "\n",
            "Тема LSA 4:\n",
            "['центр', 'искусства', 'современного', 'году', 'самый', 'москвы', 'россии', 'на', 'более', 'открылся']\n",
            "\n",
            "Тема LSA 5:\n",
            "['где', 'между', 'уникальные', 'как', 'одно', 'петра', 'для', 'исторический', 'на', 'одна']\n",
            "\n",
            "Модель spaCy 'ru_core_news_sm' не найдена.  Загрузите ее: python -m spacy download ru_core_news_sm\n",
            "\n",
            "Тема LDA 1: ['россии', 'где', 'исторический', 'музей', 'которого', 'самых', 'из', 'европе', 'музеев', 'один']\n",
            "Сущности в теме: ['NORP', 'ORG', 'PERSON', 'PERSON', 'GPE']\n",
            "\n",
            "Тема LDA 2: ['из', 'музей', 'здесь', 'музеев', 'один', 'мира', 'им', 'одно', 'москве', 'коллекций']\n",
            "Сущности в теме: ['ORG', 'PERSON', 'ORG', 'PERSON', 'PERSON', 'FAC', 'CARDINAL']\n",
            "\n",
            "Тема LDA 3: ['искусства', 'это', 'центр', 'самых', 'для', 'как', 'из', 'россии', 'музей', 'современного']\n",
            "Сущности в теме: ['GPE', 'PERSON', 'DATE', 'ORG', 'PERSON', 'GPE', 'PERSON', 'DATE', 'PERSON', 'CARDINAL', 'PERSON', 'DATE', 'ORG', 'PERSON', 'PERSON', 'ORG', 'DATE', 'CARDINAL', 'GPE']\n",
            "\n",
            "Тема LDA 4: ['она', 'не', 'музей', 'но', 'музеем', 'самым', 'является', 'москве', 'котором', 'достопримечательностей']\n",
            "Сущности в теме: ['PERSON', 'PERSON', 'GPE', 'PERSON', 'ORG']\n",
            "\n",
            "Тема LDA 5: ['на', 'музей', 'расположенный', 'был', 'москвы', 'от', 'км', 'крупным', 'является', 'самым']\n",
            "Сущности в теме: ['GPE', 'CARDINAL', 'ORG', 'DATE', 'PERSON', 'CARDINAL', 'PERSON', 'DATE', 'ORG', 'PERSON', 'GPE']\n"
          ]
        }
      ],
      "source": [
        "# Загрузка данных\n",
        "df = pd.read_csv(\"/content/cleaned_museums.csv\")\n",
        "\n",
        "# Предварительная обработка текста\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('russian')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [w for w in tokens if not w in stop_words and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "df['processed_text'] = df['Description'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# ЧАСТЬ А: Тематическое моделирование\n",
        "\n",
        "# 1. LDA\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
        "doc_term_matrix = vectorizer.fit_transform(df['Description'])\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# Вывод топ-слов для каждой темы LDA\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Тема {topic_idx + 1}:\")\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "    print(top_words)\n",
        "    print()\n",
        "\n",
        "\n",
        "# 2. LSA\n",
        "vectorizer_lsa = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "doc_term_matrix_lsa = vectorizer_lsa.fit_transform(df['Description'])\n",
        "lsa = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa.fit(doc_term_matrix_lsa)\n",
        "\n",
        "# Вывод топ-слов для каждой темы LSA (аналогично LDA)\n",
        "feature_names_lsa = vectorizer_lsa.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lsa.components_):\n",
        "    print(f\"Тема LSA {topic_idx + 1}:\")\n",
        "    top_words = [feature_names_lsa[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "    print(top_words)\n",
        "    print()\n",
        "\n",
        "# 3. NMF\n",
        "nmf = NMF(n_components=5, random_state=42) # количество тем - 5\n",
        "nmf.fit(doc_term_matrix)\n",
        "\n",
        "# ЧАСТЬ Б: Выделение именованных сущностей\n",
        "\n",
        "# Загрузка модели spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"ru_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель spaCy 'ru_core_news_sm' не найдена.  Загрузите ее: python -m spacy download ru_core_news_sm\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entities:\n",
        "            entities[ent.label_].append(ent.text)\n",
        "        else:\n",
        "            entities[ent.label_] = [ent.text]\n",
        "    return entities\n",
        "\n",
        "df['entities'] = df['Description'].apply(extract_entities)\n",
        "\n",
        "\n",
        "# Анализ именованных сущностей в каждой теме (исправленный код для LDA)\n",
        "topic_probabilities = lda.transform(doc_term_matrix)\n",
        "for i in range(lda.n_components):\n",
        "    print(f\"\\nТема LDA {i+1}: { [feature_names[j] for j in lda.components_[i].argsort()[:-10 - 1:-1]]}\")\n",
        "    topic_indices = argmax(topic_probabilities, axis=1) == i\n",
        "    entities_in_topic = []\n",
        "    for index in df[topic_indices].index:\n",
        "        entities_in_topic.extend(df['entities'][index])\n",
        "    print(\"Сущности в теме:\", entities_in_topic)\n"
      ]
    }
  ]
}