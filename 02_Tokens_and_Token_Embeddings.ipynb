{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tajibo/HomeworkDTS/blob/main/02_Tokens_and_Token_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_a9QvUFVCUR"
      },
      "source": [
        "<h1>–¢–æ–∫–µ–Ω—ã –∏ –≤–ª–æ–∂–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤</h1>\n",
        "<i>–ò–∑—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –≤–ª–æ–∂–µ–Ω–∏–π –∫–∞–∫ –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è LLM</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC4rD3ToYBM8"
      },
      "source": [
        "### [–ù–ï–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û]\n",
        "–ï—Å–ª–∏ –≤—ã –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç–µ —ç—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –≤ Google Colab (–∏–ª–∏ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º –ø–æ—Å—Ç–∞–≤—â–∏–∫–µ –æ–±–ª–∞—á–Ω—ã—Ö —É—Å–ª—É–≥), –≤–∞–º –Ω—É–∂–Ω–æ **—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å** —Å–ª–µ–¥—É—é—â–∏–π –±–ª–æ–∫ –∫–æ–¥–∞, —á—Ç–æ–±—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:\n",
        "\n",
        "---\n",
        "\n",
        "üí° **–ü–†–ò–ú–ï–ß–ê–ù–ò–ï**: –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä. –í Google Colab –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤\n",
        "**–°—Ä–µ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è > –ò–∑–º–µ–Ω–∏—Ç—å —Ç–∏–ø —Å—Ä–µ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è > –ê–ø–ø–∞—Ä–∞—Ç–Ω—ã–π —É—Å–∫–æ—Ä–∏—Ç–µ–ª—å > –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä > –¢–∏–ø –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ > T4**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-13T01:35:15.913510Z",
          "start_time": "2025-02-13T01:35:15.911465Z"
        },
        "id": "NnudTnIMYBM9"
      },
      "source": [
        "# %%capture\n",
        "# !pip install transformers>=4.41.2 sentence-transformers>=3.0.1 gensim>=4.3.2 scikit-learn>=1.5.0 accelerate>=0.31.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQHfpqT_t9-K"
      },
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∑–∞–ø—É—Å–∫ LLM\n",
        "\n",
        "–ü–µ—Ä–≤—ã–π —à–∞–≥ ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å –Ω–∞ GPU –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–≤–æ–¥–∞. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ö –∫–∞–∫ —Ç–∞–∫–æ–≤—ã–µ, —á—Ç–æ–±—ã –º—ã –º–æ–≥–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∏—Ö –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjU8NBHnwA4j",
        "ExecuteTime": {
          "end_time": "2025-02-13T01:44:50.014108Z",
          "start_time": "2025-02-13T01:35:15.987236Z"
        }
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iVl5yePuq3B"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmzgbbdKuvHt"
      },
      "outputs": [],
      "source": [
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4vsjbxwu1K1"
      },
      "outputs": [],
      "source": [
        "for id in input_ids[0]:\n",
        "   print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9wRZ3J3u4z1"
      },
      "outputs": [],
      "source": [
        "generation_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QlHLof3u8A3"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(3323))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9nRducW48bd"
      },
      "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcc3JjwX5DK-"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "üéµ È∏ü\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCDGSXP75Hv-"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ay_NX3K5HyP"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_k5QduY5H0u"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJn5nf3c5H2_"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ymhAsTg5H5e"
      },
      "outputs": [],
      "source": [
        "# The official is `tiktoken` but this the same tokenizer on the HF platform\n",
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_vAyeTy5H7_"
      },
      "outputs": [],
      "source": [
        "# You need to request access before being able to use this tokenizer\n",
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeWcUdxY6I3u"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"facebook/galactica-1.3b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__QNj2Cohzz2"
      },
      "outputs": [],
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tu7OY4HvBEm"
      },
      "source": [
        "# –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsjz-VsYu9bB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# Process the tokens\n",
        "output = model(**tokens)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQly_KcbvDce"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GcRrpPV0kVj"
      },
      "outputs": [],
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8oHVC7B0lkk"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEDuLWa0r4L"
      },
      "source": [
        "# –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Ü–µ–ª—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQHWioIc0pQ8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert text to text embeddings\n",
        "vector = model.encode(\"Best movie ever!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDwfmBiC0uER"
      },
      "outputs": [],
      "source": [
        "vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnuGRjo80yKj"
      },
      "source": [
        "# Word Embeddings –¥–æ LLMs\n"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-13T02:16:10.972023Z",
          "start_time": "2025-02-13T02:14:18.259464Z"
        },
        "id": "MUOqclc_YBNE"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKgNdnwe0vfK"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
        "# Other options include \"word2vec-google-news-300\"\n",
        "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_vj5NVn01aD"
      },
      "outputs": [],
      "source": [
        "model.most_similar([model['king']], topn=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSgyKKS4xUx"
      },
      "source": [
        "# # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–µ—Å–µ–Ω –ø–æ embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJdWzT67nDL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# Get the playlist dataset file\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# Parse the playlist dataset file. Skip the first two lines as\n",
        "# they only contain metadata\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "\n",
        "# Remove playlists with only one song\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3zirG-lo3H8"
      },
      "outputs": [],
      "source": [
        "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
        "print( 'Playlist #2:\\n ', playlists[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaUz3E0P7sJs"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train our Word2Vec model\n",
        "model = Word2Vec(\n",
        "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EFGWesO8rOJ"
      },
      "outputs": [],
      "source": [
        "song_id = 2172\n",
        "\n",
        "# Ask the model for songs similar to song #2172\n",
        "model.wv.most_similar(positive=str(song_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMiY6isXqKk4"
      },
      "outputs": [],
      "source": [
        "print(songs_df.iloc[2172])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***–î–æ–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Å–µ–Ω***"
      ],
      "metadata": {
        "id": "p7lmyWn7cO5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOzWENxr2Fl3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "    .....\n",
        "\n",
        "# Extract recommendations\n",
        "print_recommendations(2172)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqrzQQ-m1EJ5"
      },
      "outputs": [],
      "source": [
        "print_recommendations(2172)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHiN62g1NMi"
      },
      "outputs": [],
      "source": [
        "print_recommendations(842)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}